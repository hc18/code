{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e9483370588a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# --- >>> START stats <<< ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ok + failed ~ temp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfFit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;31m# --- >>> STOP stats <<< ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                        \u001b[0;34m'formula'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# attach formula for unpckling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                        'design_info': design_info})\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, family, offset, exposure, freq_weights, var_weights, missing, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                                   \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexposure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexposure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                                   \u001b[0mfreq_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                                   var_weights=var_weights, **kwargs)\n\u001b[0m\u001b[1;32m    321\u001b[0m         self._check_inputs(family, self.offset, self.exposure, self.endog,\n\u001b[1;32m    322\u001b[0m                            self.freq_weights, self.var_weights)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 64\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 633\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresettable_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# detect where the constant is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mcheck_implicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mptp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exog contains inf or nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_ptp\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     return um.subtract(\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Logistic Regression\n",
    "A logistic regression is an example of a \"Generalized Linear Model (GLM)\".\n",
    "The input values are the recorded O-ring data from the space shuttle launches before 1986,\n",
    "and the fit indicates the likelihood of failure for an O-ring.\n",
    "Taken from http://www.brightstat.com/index.php?option=com_content&task=view&id=41&Itemid=1&limit=1&limitstart=2\n",
    "'''\n",
    "\n",
    "# Copyright(c) 2015, Thomas Haslwanter. All rights reserved, under the CC BY-SA 4.0 International License\n",
    "\n",
    "# Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# additional packages\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', '..', 'Utilities'))\n",
    "\n",
    "try:\n",
    "# Import formatting commands if directory \"Utilities\" is available\n",
    "    from ISP_mystyle import setFonts, showData \n",
    "    \n",
    "except ImportError:\n",
    "# Ensure correct performance otherwise\n",
    "    def setFonts(*options):\n",
    "        return\n",
    "    def showData(*options):\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.genmod.families import Binomial\n",
    "\n",
    "sns.set_context('poster')\n",
    "\n",
    "def getData():\n",
    "    '''Get the data '''\n",
    "    \n",
    "    inFile = 'challenger_data.csv'\n",
    "    data = np.genfromtxt(inFile, skip_header=1, usecols=[1, 2],\n",
    "                                    missing_values='NA', delimiter=',')\n",
    "    # Eliminate NaNs\n",
    "    data = data[~np.isnan(data[:, 1])]\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def prepareForFit(inData):\n",
    "    ''' Make the temperature-values unique, and count the number of failures and successes.\n",
    "    Returns a DataFrame'''\n",
    "    \n",
    "    # Create a dataframe, with suitable columns for the fit\n",
    "    df = pd.DataFrame()\n",
    "    df['temp'] = np.unique(inData[:,0])\n",
    "    df['failed'] = 0\n",
    "    df['ok'] = 0\n",
    "    df['total'] = 0\n",
    "    df.index = df.temp.values\n",
    "    \n",
    "    # Count the number of starts and failures\n",
    "    for ii in range(inData.shape[0]):\n",
    "        curTemp = inData[ii,0]\n",
    "        curVal  = inData[ii,1]\n",
    "        df.loc[curTemp,'total'] += 1\n",
    "        if curVal == 1:\n",
    "            df.loc[curTemp, 'failed'] += 1\n",
    "        else:\n",
    "            df.loc[curTemp, 'ok'] += 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def logistic(x, beta, alpha=0):\n",
    "    ''' Logistic Function '''\n",
    "    return 1.0 / (1.0 + np.exp(np.dot(beta, x) + alpha))\n",
    "\n",
    "def showResults(challenger_data, model):\n",
    "    ''' Show the original data, and the resulting logit-fit'''\n",
    "    \n",
    "    temperature = challenger_data[:,0]\n",
    "    failures = challenger_data[:,1]\n",
    "    \n",
    "    # First plot the original data\n",
    "    plt.figure()\n",
    "    setFonts()\n",
    "    sns.set_style('darkgrid')\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    \n",
    "    plt.scatter(temperature, failures, s=200, color=\"k\", alpha=0.5)\n",
    "    plt.yticks([0, 1])\n",
    "    plt.ylabel(\"Damage Incident?\")\n",
    "    plt.xlabel(\"Outside Temperature [F]\")\n",
    "    plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\")\n",
    "    plt.tight_layout\n",
    "    \n",
    "    # Plot the fit\n",
    "    x = np.arange(50, 85)\n",
    "    alpha = model.params[0]\n",
    "    beta = model.params[1]\n",
    "    y = logistic(x, beta, alpha)\n",
    "    \n",
    "    plt.hold(True)\n",
    "    plt.plot(x,y,'r')\n",
    "    plt.xlim([50, 85])\n",
    "    \n",
    "    outFile = 'ChallengerPlain.png'\n",
    "    showData(outFile)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    inData = getData()\n",
    "    dfFit = prepareForFit(inData)\n",
    "    \n",
    "    # fit the model\n",
    "    \n",
    "    # --- >>> START stats <<< ---\n",
    "    model = glm('ok + failed ~ temp', data=dfFit, family=Binomial()).fit()\n",
    "    # --- >>> STOP stats <<< ---\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    showResults(inData, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of logistic ordinal regression (aka proportional odds) model\n",
    "author: fernando pedorosa, date: march-2013\n",
    "\"\"\"\n",
    "\n",
    "# Import standard packages\n",
    "import numpy as np\n",
    "\n",
    "# additional packages\n",
    "from sklearn import metrics\n",
    "from scipy import linalg, optimize, sparse\n",
    "import warnings\n",
    "\n",
    "BIG = 1e10\n",
    "SMALL = 1e-12\n",
    "\n",
    "\n",
    "def phi(t):\n",
    "    ''' logistic function, returns 1 / (1 + exp(-t)) '''\n",
    "    \n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    ''' (minus) logistic loss function, returns log(1 / (1 + exp(-t))) '''\n",
    "    \n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ordinal_logistic_fit(X, y, alpha=0, l1_ratio=0, n_class=None, max_iter=10000,\n",
    "                         verbose=False, solver='TNC', w0=None):\n",
    "    '''\n",
    "    Ordinal logistic regression or proportional odds model.\n",
    "    Uses scipy's optimize.fmin_slsqp solver.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array, sparse matrix}, shape (n_samples, n_feaures)\n",
    "        Input data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    verbose: bool\n",
    "        Print convergence information\n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (n_features,)\n",
    "        coefficients of the linear model\n",
    "    theta : array, shape (k,), where k is the different values of y\n",
    "        vector of thresholds\n",
    "    '''\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w0 = None\n",
    "\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError('Wrong shape for X and y')\n",
    "\n",
    "    # .. order input ..\n",
    "    idx = np.argsort(y)\n",
    "    idx_inv = np.zeros_like(idx)\n",
    "    idx_inv[idx] = np.arange(idx.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx].astype(np.int)\n",
    "    # make them continuous and start at zero\n",
    "    unique_y = np.unique(y)\n",
    "    for i, u in enumerate(unique_y):\n",
    "        y[y == u] = i\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # .. utility arrays used in f_grad ..\n",
    "    alpha = 0.\n",
    "    k1 = np.sum(y == unique_y[0])\n",
    "    E0 = (y[:, np.newaxis] == np.unique(y)).astype(np.int)\n",
    "    E1 = np.roll(E0, -1, axis=-1)\n",
    "    E1[:, -1] = 0.\n",
    "    E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "    def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        #loss = -(c[idx] + np.log(np.exp(-c[idx]) - 1)).sum()\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + .5 * alpha * w.dot(w) - np.log(z).sum()  # penalty\n",
    "        if np.isnan(loss):\n",
    "            pass\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "        return loss\n",
    "\n",
    "    def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "        return out\n",
    "\n",
    "    def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        import pylab as pl\n",
    "        pl.matshow(H_B)\n",
    "        pl.colorbar()\n",
    "        pl.title('True')\n",
    "        import numdifftools as nd\n",
    "        Hess = nd.Hessian(lambda x: f_obj(x, X, y))\n",
    "        H = Hess(x0)\n",
    "        pl.matshow(H[H_A.shape[0]:, H_A.shape[0]:])\n",
    "        #pl.matshow()\n",
    "        pl.title('estimated')\n",
    "        pl.colorbar()\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "    def grad_hess(x0, X, y):\n",
    "        grad = f_grad(x0, X, y)\n",
    "        hess = lambda x: f_hess(x0, x, X, y)\n",
    "        return grad, hess\n",
    "\n",
    "    x0 = np.random.randn(X.shape[1] + unique_y.size) / X.shape[1]\n",
    "    if w0 is not None:\n",
    "        x0[:X.shape[1]] = w0\n",
    "    else:\n",
    "        x0[:X.shape[1]] = 0.\n",
    "    x0[X.shape[1]:] = np.sort(unique_y.size * np.random.rand(unique_y.size))\n",
    "\n",
    "    #print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y))\n",
    "    #print(f_grad(x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y) - f_grad(x0, X, y))\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "\n",
    "    def callback(x0):\n",
    "        x0 = np.asarray(x0)\n",
    "        # print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "        if verbose:\n",
    "        # check that gradient is correctly computed\n",
    "            print('OBJ: %s' % f_obj(x0, X, y))\n",
    "\n",
    "    if solver == 'TRON':\n",
    "        import pytron\n",
    "        out = pytron.minimize(f_obj, grad_hess, x0, args=(X, y))\n",
    "    else:\n",
    "        options = {'maxiter' : max_iter, 'disp': 0, 'maxfun':10000}\n",
    "        out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "            jac=f_grad, hessp=f_hess, options=options, callback=callback)\n",
    "\n",
    "    if not out.success:\n",
    "        warnings.warn(out.message)\n",
    "    w, theta = np.split(out.x, [X.shape[1]])\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def ordinal_logistic_predict(w, theta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : coefficients obtained by ordinal_logistic\n",
    "    theta : thresholds\n",
    "    \"\"\"\n",
    "    unique_theta = np.sort(np.unique(theta))\n",
    "    out = X.dot(w)\n",
    "    unique_theta[-1] = np.inf # p(y <= max_level) = 1\n",
    "    tmp = out[:, None].repeat(unique_theta.size, axis=1)\n",
    "    return np.argmax(tmp < unique_theta, axis=1)\n",
    "\n",
    "def main():\n",
    "    DOC = \"\"\"\n",
    "================================================================================\n",
    "    Compare the prediction accuracy of different models on the boston dataset\n",
    "================================================================================\n",
    "    \"\"\"\n",
    "    print(DOC)\n",
    "    from sklearn import cross_validation, datasets\n",
    "    boston = datasets.load_boston()\n",
    "    X, y = boston.data, np.round(boston.target)\n",
    "    #X -= X.mean()\n",
    "    y -= y.min()\n",
    "\n",
    "    idx = np.argsort(y)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    cv = cross_validation.ShuffleSplit(y.size, n_iter=50, test_size=.1, random_state=0)\n",
    "    score_logistic = []\n",
    "    score_ordinal_logistic = []\n",
    "    score_ridge = []\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        #test = train\n",
    "        if not np.all(np.unique(y[train]) == np.unique(y)):\n",
    "            # we need the train set to have all different classes\n",
    "            continue\n",
    "        assert np.all(np.unique(y[train]) == np.unique(y))\n",
    "        train = np.sort(train)\n",
    "        test = np.sort(test)\n",
    "        w, theta = ordinal_logistic_fit(X[train], y[train], verbose=True,\n",
    "                                        solver='TNC')\n",
    "        pred = ordinal_logistic_predict(w, theta, X[test])\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (ORDINAL)  fold %s: %s' % (i+1, s))\n",
    "        score_ordinal_logistic.append(s)\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.LogisticRegression(C=1.)\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = clf.predict(X[test])\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (LOGISTIC) fold %s: %s' % (i+1, s))\n",
    "        score_logistic.append(s)\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.Ridge(alpha=1.)\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = np.round(clf.predict(X[test]))\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (RIDGE)    fold %s: %s' % (i+1, s))\n",
    "        score_ridge.append(s)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('MEAN ABSOLUTE ERROR (ORDINAL LOGISTIC):    %s' % np.mean(score_ordinal_logistic))\n",
    "    print('MEAN ABSOLUTE ERROR (LOGISTIC REGRESSION): %s' % np.mean(score_logistic))\n",
    "    print('MEAN ABSOLUTE ERROR (RIDGE REGRESSION):    %s' % np.mean(score_ridge))\n",
    "    # print('Chance level is at %s' % (1. / np.unique(y).size))\n",
    "    \n",
    "    return np.mean(score_ridge)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    out = main()    \n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
