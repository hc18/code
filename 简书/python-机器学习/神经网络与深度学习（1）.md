####1. 绪论
![image.png](https://upload-images.jianshu.io/upload_images/6634703-9e29b9e492981749.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####2. 损失函数
1. 0-1 损失函数
![image.png](https://upload-images.jianshu.io/upload_images/6634703-042cef48c172c470.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
2. 平方损失函数
![image.png](https://upload-images.jianshu.io/upload_images/6634703-8c24d406c9577f6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
3. 交叉商损失函数（用于分类问题）
![image.png](https://upload-images.jianshu.io/upload_images/6634703-e1f1c3eb52485c93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
4. Hinge 损失函数（用于两类分类问题）
![](https://upload-images.jianshu.io/upload_images/6634703-5ddef678c8945014.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####3. 优化算法
1. 梯度下降法（容易跑到局部最优解）
![image.png](https://upload-images.jianshu.io/upload_images/6634703-3e6c3ced36f0ffcf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
2. 批量梯度下降法（样本量N多，迭代开销大）
![image.png](https://upload-images.jianshu.io/upload_images/6634703-c33620893dc40954.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
3. 随机梯度下降法（比较合适）
![image.png](https://upload-images.jianshu.io/upload_images/6634703-2261f09410d6aae6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-20875822f58b063e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 批量梯度下降和随机梯度下降之间的区别在于每次迭代的优化目标是对所有样本的平均损失函数还是单个样本的损失函数。
4. 小批量梯度下降法
- 在实际应用中，小批量随机梯度下降方法有收敛快，计算开销小的优点，因此逐渐成为大规模的机器学习中的主要优化算法
![image.png](https://upload-images.jianshu.io/upload_images/6634703-441b9f012effd01d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




