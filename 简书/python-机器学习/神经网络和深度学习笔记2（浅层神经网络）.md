####1. 浅层神经网络
![image.png](https://upload-images.jianshu.io/upload_images/6634703-6e7e5ba7e27402e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-e3fd0ebf6c7d8047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 损失函数从右向左传递
![image.png](https://upload-images.jianshu.io/upload_images/6634703-440e684c09994c9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####2. 神经网络的计算
- 关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表 示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出𝑧，然后在第二 步中你以 sigmoid 函数为激活函数计算𝑧(得出𝑎)，一个神经网络只是这样子做了好多次重 复计算。
![image.png](https://upload-images.jianshu.io/upload_images/6634703-72f31cee7f058343.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-a7c6b7c5b7e50ab5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 多样本向量化
![image.png](https://upload-images.jianshu.io/upload_images/6634703-f6327ea96e0e5e40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-71bbe6ec463d8715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####3. 激活函数
- sigmoid && tanh
- 使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。 到目前为止，之前的视频只用过 sigmoid 激活函数，但是，有时其他的激活函数效果会更好。
![image.png](https://upload-images.jianshu.io/upload_images/6634703-06b7d97e677beb66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-4d032a6b3e3116ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- ReLu
![image.png](https://upload-images.jianshu.io/upload_images/6634703-bf72a46507a4cc33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-335320f6ea4930e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 两者的优点是：
- 第一，在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中， 使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。
- 第二，sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。(同时应该注 意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性， 而 Leaky ReLu 不会有这问题)
- 第三， 𝑧在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的 训练数据来说学习过程仍然可以很快。
![image.png](https://upload-images.jianshu.io/upload_images/6634703-a57d6f62f094e23d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####4 激活函数的导数
- 在神经网络中使用**反向传播**的时候，你真的需要计算激活函数的斜率或者导数。针对以 下四种激活，求其导数如下
![image.png](https://upload-images.jianshu.io/upload_images/6634703-9d855f83dc14c3e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-2fc31ce3628a006b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-dccbc3da5e3401a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-6f825c54d26df887.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####5. 神经网络的梯度下降
![image.png](https://upload-images.jianshu.io/upload_images/6634703-ebe87efa3e084370.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-d4939a93c8dd5cfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####6. 直观理解反向传播
![image.png](https://upload-images.jianshu.io/upload_images/6634703-108d8ceb3ba8c659.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/6634703-4263e4fc46999b5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
####7. 随机初始化
- 对于一个神经网络，如果你把权重或者参数都初始化为 0，那么梯度下降将不会起作用。
- 一般设置
![image.png](https://upload-images.jianshu.io/upload_images/6634703-4b5141aa0943656a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
